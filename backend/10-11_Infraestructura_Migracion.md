# Plan de MigraciÃ³n HSE Backend - Secciones 10-11

**ContinuaciÃ³n desde**: `07-09_CQRS_Patrones_Estructura.md`

---

## 10. Infraestructura Cloud (AWS)

### 10.1 Arquitectura en AWS

```
                    INTERNET
                       â”‚
                       â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  CloudFront   â”‚ CDN + DDoS protection
               â”‚    (CDN)      â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚      ALB      â”‚ Application Load Balancer
               â”‚ (Layer 7 LB)  â”‚ SSL Termination
               â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                     â”‚
            â–¼                     â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ECS Fargate  â”‚      â”‚ ECS Fargate  â”‚
    â”‚  (Monolith)  â”‚â—„â”€â”€â”€â”€â–ºâ”‚  (Devices)   â”‚
    â”‚ Auto-scaling â”‚ gRPC â”‚ Auto-scaling â”‚
    â”‚  2-10 tasks  â”‚      â”‚  2-8 tasks   â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                     â”‚
           â”‚                     â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚
        â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚RDS PostgreSQLâ”‚      â”‚ ElastiCache  â”‚
â”‚   Multi-AZ   â”‚      â”‚    Redis     â”‚
â”‚+ TimescaleDB â”‚      â”‚(Cluster Mode)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  CloudWatch   â”‚ Logs + Metrics
           â”‚+ New Relic    â”‚ APM + Traces
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚    Secrets    â”‚ Credentials
           â”‚   Manager     â”‚ + Rotation
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 10.2 Servicios AWS Detallados

#### 10.2.1 ECS Fargate (Compute)

**Monolito (NestJS)**:
```yaml
Service: hse-monolith
Cluster: hse-cluster

Task Definition:
  CPU: 1024 (1 vCPU)
  Memory: 2048 MB (2 GB)
  Container:
    Image: <account>.dkr.ecr.us-east-1.amazonaws.com/hse-monolith:latest
    Port: 8080
    Environment:
      - NODE_ENV=production
      - NEW_RELIC_LICENSE_KEY=from-secrets-manager
    HealthCheck:
      Command: ["CMD-SHELL", "curl -f http://localhost:8080/health/readiness || exit 1"]
      Interval: 30s
      Timeout: 5s
      Retries: 3

Service Config:
  Desired Count: 2
  Min Capacity: 2
  Max Capacity: 10
  
  Auto-scaling:
    Target CPU: 70%
    Target Memory: 80%
    Scale-out cooldown: 60s
    Scale-in cooldown: 300s
  
  Load Balancer:
    Target Group: hse-monolith-tg
    Health Check Path: /health/readiness
    Healthy Threshold: 2
    Unhealthy Threshold: 3
```

**Devices Service (Go)**:
```yaml
Service: hse-devices
Cluster: hse-cluster

Task Definition:
  CPU: 2048 (2 vCPU)
  Memory: 4096 MB (4 GB)
  Container:
    Image: <account>.dkr.ecr.us-east-1.amazonaws.com/hse-devices:latest
    Ports:
      - 8080 (HTTP)
      - 50051 (gRPC)
      - 5000 (TCP devices)
    Environment:
      - ENVIRONMENT=production
    HealthCheck:
      Command: ["CMD-SHELL", "curl -f http://localhost:8080/health/readiness || exit 1"]
      Interval: 30s
      Timeout: 5s
      Retries: 3

Service Config:
  Desired Count: 2
  Min Capacity: 2
  Max Capacity: 8
  
  Auto-scaling:
    Target Metric: Active Connections
    Target Value: 1000 connections
    Scale-out cooldown: 60s
    Scale-in cooldown: 300s
```

**Por quÃ© Fargate y no EC2**:
- Sin gestiÃ³n de servers (serverless containers)
- Auto-scaling automÃ¡tico
- Pago por uso (solo por recursos usados)
- Security patches automÃ¡ticos
- Menos complejidad operacional

#### 10.2.2 RDS PostgreSQL + TimescaleDB

**ConfiguraciÃ³n Staging**:
```yaml
Instance Class: db.t3.medium
  vCPU: 2
  RAM: 4 GB
  
Storage:
  Type: gp3 (SSD)
  Size: 100 GB
  IOPS: 3000
  Auto-scaling: Enabled (up to 500 GB)

Multi-AZ: No (costo reducido)
Backup Retention: 7 dÃ­as
```

**ConfiguraciÃ³n Production**:
```yaml
Instance Class: db.r6g.xlarge
  vCPU: 4
  RAM: 32 GB
  
Storage:
  Type: gp3 (SSD)
  Size: 500 GB
  IOPS: 12000
  Auto-scaling: Enabled (up to 1 TB)

Multi-AZ: Yes (alta disponibilidad)
  Primary: us-east-1a
  Standby: us-east-1b
  Failover: AutomÃ¡tico (< 60 segundos)

Backups:
  Automated: Daily (retenciÃ³n 30 dÃ­as)
  Point-in-time recovery: Enabled
  Snapshots: Manual antes de migraciones

Security:
  Encryption at rest: AES-256
  Encryption in transit: SSL/TLS
  
Parameter Group:
  shared_preload_libraries: 'timescaledb'
  max_connections: 200
  work_mem: 16MB
```

**InstalaciÃ³n de TimescaleDB**:
```sql
-- 1. Conectar como superuser
psql -h your-rds-endpoint.rds.amazonaws.com -U postgres -d hse_production

-- 2. Crear extensiÃ³n (requiere rds_superuser role)
CREATE EXTENSION IF NOT EXISTS timescaledb CASCADE;

-- 3. Verificar instalaciÃ³n
SELECT extname, extversion FROM pg_extension WHERE extname = 'timescaledb';

-- 4. Configurar continuous aggregates refresh
SELECT add_continuous_aggregate_policy('device_events_hourly',
  start_offset => INTERVAL '3 hours',
  end_offset => INTERVAL '1 hour',
  schedule_interval => INTERVAL '10 minutes'
);
```

#### 10.2.3 ElastiCache Redis

**ConfiguraciÃ³n Production**:
```yaml
Cluster Mode: Enabled
Node Type: cache.r6g.large
  vCPU: 2
  RAM: 13.07 GB

Topology:
  Shards: 2
  Replicas per shard: 2
  Total nodes: 6 (2 primary + 4 replicas)
  Total capacity: ~78 GB

Configuration:
  Engine Version: 7.0
  Port: 6379
  
Security:
  Encryption at rest: Yes
  Encryption in transit: Yes
  Auth token: Enabled
  
  Security Group:
    Inbound: Port 6379 from ECS-SG only

Automatic Failover: Enabled
Backup:
  Daily snapshot
  Retention: 7 dÃ­as

Maintenance Window: sun:03:00-sun:04:00
```

**Uso de Redis**:
```
Shard 1 (Queue + DLQ):
â”œâ”€ events:queue         â†’ Cola de eventos (LPUSH/BRPOP)
â”œâ”€ events:dlq           â†’ Dead Letter Queue
â””â”€ events:dlq:permanent â†’ DLQ permanente

Shard 2 (Cache):
â”œâ”€ device:state:*       â†’ Estados actuales (TTL: 1 min)
â”œâ”€ device:agg:*         â†’ Agregaciones (TTL: 5 min)
â””â”€ session:*            â†’ Sesiones de usuarios
```

#### 10.2.4 Application Load Balancer (ALB)

```yaml
Scheme: Internet-facing
IP Address Type: ipv4

Listeners:
  - Port: 443 (HTTPS)
    Protocol: HTTPS
    SSL Certificate: ACM Certificate
    Default Action: Forward to hse-monolith-tg
    
  - Port: 80 (HTTP)
    Protocol: HTTP
    Default Action: Redirect to HTTPS

Target Groups:
  hse-monolith-tg:
    Protocol: HTTP
    Port: 8080
    Health Check:
      Path: /health/readiness
      Interval: 30s
      Timeout: 5s
      Healthy Threshold: 2
      Unhealthy Threshold: 3
      Matcher: 200
    
  hse-devices-tg:
    Protocol: HTTP
    Port: 8080
    Health Check:
      Path: /health/readiness
      Interval: 30s
      Timeout: 5s
      Healthy Threshold: 2
      Unhealthy Threshold: 3
      Matcher: 200

Security Groups:
  ALB-SG:
    Inbound:
      - Port 443 from 0.0.0.0/0
      - Port 80 from 0.0.0.0/0
    Outbound:
      - Port 8080 to ECS-SG

Attributes:
  Idle timeout: 60 seconds
  Cross-zone load balancing: Enabled
  Access logs: Enabled (S3 bucket)
```

#### 10.2.5 VPC y Networking

```yaml
VPC:
  CIDR: 10.0.0.0/16
  DNS Hostnames: Enabled
  DNS Resolution: Enabled

Subnets:
  Public Subnets (ALB):
    - public-subnet-1a: 10.0.1.0/24 (us-east-1a)
    - public-subnet-1b: 10.0.2.0/24 (us-east-1b)
    
  Private Subnets (ECS):
    - private-subnet-1a: 10.0.10.0/24 (us-east-1a)
    - private-subnet-1b: 10.0.11.0/24 (us-east-1b)
    
  Database Subnets:
    - db-subnet-1a: 10.0.20.0/24 (us-east-1a)
    - db-subnet-1b: 10.0.21.0/24 (us-east-1b)

Internet Gateway:
  Attached to VPC
  Routes: Public subnets â†’ IGW

NAT Gateway:
  Location: public-subnet-1a
  Elastic IP: Allocated
  Routes: Private subnets â†’ NAT

Security Groups:
  ALB-SG:
    Inbound: 443, 80 from 0.0.0.0/0
    Outbound: 8080 to ECS-SG
  
  ECS-SG:
    Inbound: 8080 from ALB-SG, 50051 from ECS-SG
    Outbound: 5432 to DB-SG, 6379 to Redis-SG, All to 0.0.0.0/0 (Internet)
  
  DB-SG:
    Inbound: 5432 from ECS-SG
    Outbound: None
  
  Redis-SG:
    Inbound: 6379 from ECS-SG
    Outbound: None
```

#### 10.2.6 Secrets Manager

```json
{
  "database": {
    "host": "hse-prod.c9dnexample.us-east-1.rds.amazonaws.com",
    "port": 5432,
    "username": "hse_admin",
    "password": "auto-generated-secure-password",
    "database": "hse_production"
  },
  "redis": {
    "host": "hse-prod.cache.amazonaws.com",
    "port": 6379,
    "password": "auto-generated-auth-token"
  },
  "jwt": {
    "secret": "auto-generated-jwt-secret",
    "expiresIn": "15m",
    "refreshExpiresIn": "7d"
  },
  "newrelic": {
    "licenseKey": "your-newrelic-license-key",
    "appName": "HSE Backend Production"
  },
  "devices_grpc": {
    "url": "hse-devices.local:50051"
  }
}
```

**RotaciÃ³n automÃ¡tica**:
```yaml
Rotation Policy:
  Enabled: Yes
  Rotation Interval: 30 dÃ­as
  Lambda Function: auto-rotation-function
```

### 10.3 CI/CD Pipeline (GitHub Actions)

**Workflow completo** (`.github/workflows/deploy.yml`):
```yaml
name: Deploy to AWS

on:
  push:
    branches: [main, develop]

env:
  AWS_REGION: us-east-1
  ENVIRONMENT: ${{ github.ref == 'refs/heads/main' && 'production' || 'staging' }}

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Test Monolith
        run: |
          cd hse-backend-monolith
          npm ci
          npm run test
          npm run test:e2e
      
      - name: Test Devices
        run: |
          cd hse-devices-service
          go test ./... -v

  build-and-push:
    needs: test
    runs-on: ubuntu-latest
    outputs:
      monolith-image: ${{ steps.meta-monolith.outputs.tags }}
      devices-image: ${{ steps.meta-devices.outputs.tags }}
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Login to ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1
      
      - name: Build and push Monolith
        uses: docker/build-push-action@v4
        with:
          context: ./hse-backend-monolith
          push: true
          tags: |
            ${{ steps.login-ecr.outputs.registry }}/hse-monolith:${{ github.sha }}
            ${{ steps.login-ecr.outputs.registry }}/hse-monolith:latest
      
      - name: Build and push Devices
        uses: docker/build-push-action@v4
        with:
          context: ./hse-devices-service
          push: true
          tags: |
            ${{ steps.login-ecr.outputs.registry }}/hse-devices:${{ github.sha }}
            ${{ steps.login-ecr.outputs.registry }}/hse-devices:latest

  deploy:
    needs: build-and-push
    runs-on: ubuntu-latest
    steps:
      - name: Deploy Monolith to ECS
        run: |
          aws ecs update-service \
            --cluster hse-cluster \
            --service hse-monolith-${{ env.ENVIRONMENT }} \
            --force-new-deployment \
            --region ${{ env.AWS_REGION }}
      
      - name: Deploy Devices to ECS
        run: |
          aws ecs update-service \
            --cluster hse-cluster \
            --service hse-devices-${{ env.ENVIRONMENT }} \
            --force-new-deployment \
            --region ${{ env.AWS_REGION }}
      
      - name: Wait for deployment
        run: |
          aws ecs wait services-stable \
            --cluster hse-cluster \
            --services hse-monolith-${{ env.ENVIRONMENT }} hse-devices-${{ env.ENVIRONMENT }} \
            --region ${{ env.AWS_REGION }}
      
      - name: Verify health
        run: |
          curl -f https://api-${{ env.ENVIRONMENT }}.hse.com/health/readiness || exit 1

  notify:
    needs: [test, build-and-push, deploy]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Notify team
        run: |
          # Send Slack notification
          curl -X POST ${{ secrets.SLACK_WEBHOOK_URL }} \
            -H 'Content-Type: application/json' \
            -d '{
              "text": "Deploy ${{ needs.deploy.result }} - ${{ env.ENVIRONMENT }}",
              "attachments": [{
                "color": "${{ needs.deploy.result == 'success' && 'good' || 'danger' }}",
                "fields": [{
                  "title": "Environment",
                  "value": "${{ env.ENVIRONMENT }}",
                  "short": true
                }, {
                  "title": "Commit",
                  "value": "${{ github.sha }}",
                  "short": true
                }]
              }]
            }'
```

### 10.4 Costos Estimados Mensuales

#### Staging Environment
```
ECS Fargate:
â”œâ”€ Monolith: 1 task Ã— 1 vCPU Ã— 2 GB Ã— 730h = $40
â””â”€ Devices: 1 task Ã— 2 vCPU Ã— 4 GB Ã— 730h = $75

RDS PostgreSQL:
â””â”€ db.t3.medium (100 GB storage) = $120

ElastiCache Redis:
â””â”€ cache.t3.small (1 node) = $50

ALB:
â””â”€ Load Balancer + Data Transfer = $25

Data Transfer:
â””â”€ Outbound (estimado) = $20

CloudWatch Logs:
â””â”€ Ingestion + Storage = $15

Secrets Manager:
â””â”€ 5 secrets = $2

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL STAGING: ~$347/mes
```

#### Production Environment
```
ECS Fargate (promedio 4 tasks cada uno):
â”œâ”€ Monolith: 4 Ã— 1 vCPU Ã— 2 GB Ã— 730h = $160
â””â”€ Devices: 3 Ã— 2 vCPU Ã— 4 GB Ã— 730h = $225

RDS PostgreSQL:
â””â”€ db.r6g.xlarge Multi-AZ (500 GB) = $850

ElastiCache Redis:
â””â”€ cache.r6g.large cluster (6 nodes) = $450

ALB:
â””â”€ Load Balancer + Data Transfer = $60

Data Transfer:
â””â”€ Outbound (estimado 500 GB/mes) = $45

CloudWatch Logs:
â””â”€ Ingestion (50 GB) + Storage = $30

New Relic:
â””â”€ Pro plan (100 GB/mes) = $99

Secrets Manager:
â””â”€ 10 secrets + rotations = $5

Backups (RDS snapshots):
â””â”€ Snapshots storage = $20

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL PRODUCTION: ~$1,944/mes
```

**Optimizaciones de costos**:
- Reserved Instances para RDS (ahorro 30-40%)
- Savings Plans para Fargate (ahorro 20%)
- Lifecycle policies para logs (retenciÃ³n 30 dÃ­as)
- S3 Intelligent-Tiering para backups

---

## 11. Estrategia de MigraciÃ³n (Strangler Fig Pattern)

### 11.1 Principios Fundamentales

**Strangler Fig Pattern**: MigraciÃ³n incremental que "estrangula" gradualmente al sistema viejo.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Sistema Viejo                      â”‚
â”‚  (Monolito actual con todas las features)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Fase 1: Coexistencia                    â”‚
â”‚   Viejo (100%) + Nuevo (shadow mode)           â”‚
â”‚   â”‚                                             â”‚
â”‚   â”œâ”€ Nuevo recibe copia de eventos             â”‚
â”‚   â””â”€ ComparaciÃ³n de resultados                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Fase 2: Traffic Split                   â”‚
â”‚   Viejo (90%) + Nuevo (10%)                    â”‚
â”‚   â”‚                                             â”‚
â”‚   â”œâ”€ Routing por feature flags                 â”‚
â”‚   â”œâ”€ Monitoreo intensivo                       â”‚
â”‚   â””â”€ Rollback rÃ¡pido si hay issues             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Fase 3: Incremento Gradual              â”‚
â”‚   25% â†’ 50% â†’ 75% â†’ 100%                       â”‚
â”‚   â”‚                                             â”‚
â”‚   â””â”€ ValidaciÃ³n continua de mÃ©tricas           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Sistema Nuevo                      â”‚
â”‚   (100% de trÃ¡fico) + Viejo en standby        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Por quÃ© este enfoque**:
- **Riesgo minimizado**: Podemos hacer rollback en cualquier momento
- **ValidaciÃ³n continua**: Comparamos comportamiento entre sistemas
- **Sin downtime**: Ambos sistemas corren en paralelo
- **Aprendizaje iterativo**: Cada fase nos da feedback

### 11.2 Fases Detalladas

#### Fase 0: PreparaciÃ³n (Semanas 1-2)

**Objetivo**: Infraestructura lista y operacional.

**Tareas**:
```
â–¡ Infraestructura AWS
  â”œâ”€ Crear VPC con subnets
  â”œâ”€ Provisionar RDS PostgreSQL
  â”œâ”€ Instalar TimescaleDB extension
  â”œâ”€ Provisionar ElastiCache Redis
  â”œâ”€ Configurar Security Groups
  â””â”€ Setup ALB

â–¡ CI/CD
  â”œâ”€ Configurar GitHub Actions
  â”œâ”€ Setup ECR repositories
  â”œâ”€ Test pipeline en staging
  â””â”€ Documentar proceso de deploy

â–¡ Monitoring
  â”œâ”€ Configurar New Relic
  â”œâ”€ Setup CloudWatch dashboards
  â”œâ”€ Configurar alertas crÃ­ticas
  â””â”€ Test de notificaciones

â–¡ Secrets
  â”œâ”€ Migrar credenciales a Secrets Manager
  â”œâ”€ Configurar rotaciÃ³n automÃ¡tica
  â””â”€ Test de acceso desde ECS
```

**Criterios de Ã©xito**:
- âœ… Deploy automÃ¡tico funciona end-to-end
- âœ… Health checks responden correctamente
- âœ… Logs llegan a CloudWatch y New Relic
- âœ… Alertas se disparan correctamente

#### Fase 1: Devices Service en Shadow Mode (Semanas 3-4)

**Objetivo**: Devices Service corriendo en producciÃ³n pero sin servir trÃ¡fico real.

**Estrategia**:
```
Evento de dispositivo
    â”‚
    â”œâ”€â”€> Sistema Viejo (guarda en su DB)
    â”‚
    â””â”€â”€> Sistema Nuevo (guarda en TimescaleDB)
         â””â”€ ComparaciÃ³n automÃ¡tica cada hora
```

**Tareas**:
```
â–¡ Deploy Devices Service
  â”œâ”€ Deploy a staging
  â”œâ”€ Tests de carga (simular 200 eventos/seg)
  â”œâ”€ Deploy a production
  â””â”€ Verificar health checks

â–¡ Dual Write Mode
  â”œâ”€ Configurar webhook en sistema viejo
  â”œâ”€ Enviar copia de eventos a Devices Service
  â”œâ”€ NO usar datos del nuevo sistema aÃºn
  â””â”€ Solo guardar y comparar

â–¡ Script de ValidaciÃ³n
  â”œâ”€ Comparar conteo de eventos por hora
  â”œâ”€ Comparar valores de eventos
  â”œâ”€ Detectar discrepancias
  â””â”€ Alertar si drift > 1%

â–¡ Monitoreo
  â”œâ”€ Dashboard de comparaciÃ³n
  â”œâ”€ Latencia de escritura (debe ser < 10ms)
  â”œâ”€ Batch processing lag (debe ser < 5 seg)
  â””â”€ DLQ size (debe ser ~0)
```

**Script de validaciÃ³n**:
```bash
#!/bin/bash
# scripts/validate-shadow-mode.sh

START_DATE="2024-01-04 00:00:00"
END_DATE="2024-01-04 23:59:59"

# Contar eventos en sistema viejo
OLD_COUNT=$(psql $OLD_DB -t -c "
  SELECT COUNT(*) FROM device_events 
  WHERE time BETWEEN '$START_DATE' AND '$END_DATE'
")

# Contar eventos en sistema nuevo
NEW_COUNT=$(psql $NEW_DB -t -c "
  SELECT COUNT(*) FROM device_events 
  WHERE time BETWEEN '$START_DATE' AND '$END_DATE'
")

# Calcular drift
DRIFT=$(echo "scale=2; (1 - $NEW_COUNT / $OLD_COUNT) * 100" | bc)

echo "Old system: $OLD_COUNT events"
echo "New system: $NEW_COUNT events"
echo "Drift: $DRIFT%"

if (( $(echo "$DRIFT > 1" | bc -l) )); then
  echo "âŒ ALERT: Drift > 1%"
  exit 1
else
  echo "âœ… OK: Systems in sync"
fi
```

**Criterios de Ã©xito**:
- âœ… 99.9%+ de eventos coinciden entre sistemas
- âœ… Latencia de escritura < 10ms
- âœ… Batch processing lag < 5 segundos
- âœ… DLQ vacÃ­a o casi vacÃ­a
- âœ… Sin errores en logs por 7 dÃ­as consecutivos

#### Fase 2: MigraciÃ³n MÃ³dulo IoT/Core (Semanas 5-7)

**Objetivo**: Migrar lÃ³gica de negocio del mÃ³dulo IoT/Core con traffic split gradual.

**Semana 5: Traffic Split 10%**
```
Feature Flag: traffic_split_iot_core = 10

ImplementaciÃ³n:
- Hash del user ID determina routing
- 10% de usuarios â†’ Sistema nuevo
- 90% de usuarios â†’ Sistema viejo
- Ambos sistemas registran mÃ©tricas
```

**ImplementaciÃ³n de Traffic Split**:
```typescript
// src/shared/application/interceptors/traffic-split.interceptor.ts
@Injectable()
export class TrafficSplitInterceptor implements NestInterceptor {
  constructor(
    private configService: ConfigService,
    private metricsService: MetricsService,
  ) {}

  async intercept(context: ExecutionContext, next: CallHandler) {
    const request = context.switchToHttp().getRequest();
    const userId = request.user?.id;
    
    // Obtener porcentaje de traffic para nuevo sistema
    const splitPercent = await this.configService.get('TRAFFIC_SPLIT_PERCENT');
    
    // Determinar si usar nuevo sistema
    const useNewSystem = this.shouldUseNewSystem(userId, splitPercent);
    
    if (useNewSystem) {
      request.useNewSystem = true;
      this.metricsService.increment('traffic_split.new_system');
    } else {
      request.useNewSystem = false;
      this.metricsService.increment('traffic_split.old_system');
    }
    
    const startTime = Date.now();
    
    try {
      const result = await next.handle().toPromise();
      
      const duration = Date.now() - startTime;
      this.metricsService.histogram(
        useNewSystem ? 'latency.new_system' : 'latency.old_system',
        duration
      );
      
      return result;
    } catch (error) {
      this.metricsService.increment(
        useNewSystem ? 'errors.new_system' : 'errors.old_system'
      );
      throw error;
    }
  }

  private shouldUseNewSystem(userId: string, percent: number): boolean {
    if (!userId) return false;
    
    // Hash consistente basado en user ID
    const hash = crypto.createHash('md5').update(userId).digest('hex');
    const hashInt = parseInt(hash.substring(0, 8), 16);
    
    return (hashInt % 100) < percent;
  }
}
```

**Monitoreo durante traffic split**:
```
Dashboards crÃ­ticos:
â”œâ”€ Error rate (nuevo vs viejo)
â”œâ”€ Latency p50, p95, p99 (nuevo vs viejo)
â”œâ”€ Throughput (requests/min)
â”œâ”€ Discrepancias en respuestas
â””â”€ User complaints (support tickets)

Alertas configuradas:
â”œâ”€ Error rate nuevo > error rate viejo + 0.5%
â”œâ”€ Latency p99 nuevo > latency p99 viejo Ã— 1.5
â””â”€ Discrepancy rate > 1%
```

**Semana 6: Incremento a 50%**

Si mÃ©tricas son buenas en semana 5:
```
Feature Flag: traffic_split_iot_core = 50

ValidaciÃ³n:
â–¡ Error rate nuevo â‰ˆ error rate viejo
â–¡ Latency nuevo â‰¤ latency viejo
â–¡ Sin incremento en support tickets
â–¡ Feedback de usuarios positivo/neutral
```

**Semana 7: Incremento a 100%**

Si semana 6 es exitosa:
```
Feature Flag: traffic_split_iot_core = 100

Todos los usuarios â†’ Sistema nuevo
Sistema viejo en standby (ready para rollback)
```

**Plan de Rollback**:
```bash
# Si hay problemas CRÃTICOS en cualquier momento:

# 1. Reducir trÃ¡fico inmediatamente (< 30 segundos)
aws ssm put-parameter \
  --name /hse/traffic-split-percent \
  --value "0" \
  --overwrite

# 2. Verificar que trÃ¡fico vuelve a sistema viejo
curl https://api.hse.com/health

# 3. Investigar issue en nuevo sistema sin presiÃ³n

# 4. Comunicar a stakeholders
```

**Criterios de Ã©xito Fase 2**:
- âœ… Error rate < 0.1%
- âœ… Latency p99 < 500ms
- âœ… 7 dÃ­as de operaciÃ³n estable al 100%
- âœ… Sin incidentes crÃ­ticos
- âœ… Feedback positivo de usuarios

#### Fase 3: MigraciÃ³n MÃ³dulo Booking (Semanas 8-9)

**Objetivo**: Migrar sistema de reservas.

**Estrategia**: Similar a IoT/Core pero con validaciones adicionales.

**Semana 8: Traffic Split 10-50%**
```
Validaciones especÃ­ficas:
â”œâ”€ Sin reservas duplicadas
â”œâ”€ Sin solapamientos invÃ¡lidos
â”œâ”€ CÃ³digos de acceso funcionando
â”œâ”€ Notificaciones enviadas correctamente
â””â”€ Transiciones de estado correctas
```

**Datos histÃ³ricos** (opcional):
```sql
-- Migrar solo reservas activas y futuras
-- Reservas pasadas (completed) no son necesarias

INSERT INTO new_system.bookings
SELECT * FROM old_system.bookings
WHERE status IN ('pending', 'confirmed', 'in-progress')
   OR check_out_date >= CURRENT_DATE;

-- Validar integridad
SELECT 
  COUNT(*) as total,
  status,
  COUNT(DISTINCT guest_id) as unique_guests
FROM new_system.bookings
GROUP BY status;
```

**Semana 9: Traffic Split 100%**

**Criterios de Ã©xito**:
- âœ… Sin reservas duplicadas en 7 dÃ­as
- âœ… CÃ³digos de acceso 100% funcionales
- âœ… Sin quejas de huÃ©spedes
- âœ… Todas las notificaciones enviadas

#### Fase 4: Apagar Sistema Viejo (Semana 10+)

**Objetivo**: Eliminar sistema viejo completamente.

**Precondiciones**:
```
â–¡ 2 semanas de operaciÃ³n estable al 100%
â–¡ Sin incidentes crÃ­ticos
â–¡ Feedback positivo de usuarios
â–¡ MÃ©tricas dentro de rangos esperados
â–¡ Equipo confiado en nuevo sistema
```

**Proceso**:
```
DÃ­a 1-7: Monitoreo intensivo
â”œâ”€ On-call 24/7
â”œâ”€ RevisiÃ³n diaria de mÃ©tricas
â””â”€ ReuniÃ³n diaria de status

DÃ­a 8: Backup completo sistema viejo
â”œâ”€ Snapshot de base de datos
â”œâ”€ Export de configuraciones
â””â”€ Documentar procedimiento de rollback

DÃ­a 9: Sistema viejo en read-only
â”œâ”€ Deshabilitar escrituras
â”œâ”€ Mantener disponible para consultas
â””â”€ Monitorear si se usa

DÃ­a 10-16: ValidaciÃ³n final
â”œâ”€ Confirmar que nadie usa sistema viejo
â”œâ”€ Validar integridad de datos
â””â”€ AprobaciÃ³n de stakeholders

DÃ­a 17: Apagar sistema viejo
â”œâ”€ Stop de servicios
â”œâ”€ Mantener backups por 30 dÃ­as
â””â”€ Documentar apagado
```

**Rollback de emergencia** (si es absolutamente necesario):
```bash
#!/bin/bash
# scripts/emergency-rollback.sh

echo "ğŸš¨ EMERGENCY ROLLBACK ğŸš¨"

# 1. Reactivar sistema viejo
aws ecs update-service \
  --cluster hse-cluster \
  --service hse-old-system \
  --desired-count 4

# 2. Traffic split a 0
aws ssm put-parameter \
  --name /hse/use-new-system \
  --value "false" \
  --overwrite

# 3. Esperar a que sistema viejo estÃ© ready
aws ecs wait services-stable \
  --cluster hse-cluster \
  --services hse-old-system

# 4. Verificar salud
curl -f https://api-old.hse.com/health || exit 1

echo "âœ… Rollback completado"
echo "âš ï¸  Investigar issue en nuevo sistema"
```

### 11.3 Checklist de MigraciÃ³n

#### Pre-MigraciÃ³n
```
â–¡ Backup completo del sistema viejo
â–¡ Plan de rollback documentado y probado
â–¡ Feature flags configurados y testeados
â–¡ Monitoring dashboards listos
â–¡ Alertas configuradas en New Relic
â–¡ On-call rotation definida
â–¡ ComunicaciÃ³n a stakeholders programada
â–¡ Ventana de mantenimiento (si es necesaria)
â–¡ Runbook de incidentes actualizado
```

#### Durante MigraciÃ³n
```
â–¡ Monitoreo activo 24/7
â–¡ Daily standup con equipo tÃ©cnico
â–¡ ComparaciÃ³n automÃ¡tica de respuestas
â–¡ Logs centralizados y accesibles
â–¡ MÃ©tricas en tiempo real visibles
â–¡ Canal de comunicaciÃ³n de emergencia abierto
â–¡ Feedback de usuarios monitoreado
â–¡ Traffic split ajustable en tiempo real
```

#### Post-MigraciÃ³n
```
â–¡ AnÃ¡lisis de mÃ©tricas vs baseline
â–¡ Documento de lecciones aprendidas
â–¡ Identificar optimizaciones
â–¡ Documentar deuda tÃ©cnica
â–¡ Celebrar con el equipo ğŸ‰
â–¡ Comunicar Ã©xito a stakeholders
â–¡ Planear prÃ³ximas mejoras
```

### 11.4 MÃ©tricas de Ã‰xito

**MÃ©tricas tÃ©cnicas**:
```
Uptime:
Target: > 99.9%
Current: (se mide post-migraciÃ³n)

Error Rate:
Target: < 0.1%
Baseline viejo: 0.15%
Current: (se mide post-migraciÃ³n)

Response Time (p99):
Target: < 500ms
Baseline viejo: 650ms
Current: (se mide post-migraciÃ³n)

Eventos Procesados:
Target: 100+ eventos/segundo sin lag
Lag aceptable: < 5 segundos
Current: (se mide post-migraciÃ³n)

Data Loss:
Target: 0 eventos perdidos
MÃ©todo: ValidaciÃ³n automÃ¡tica cada hora
Current: (se mide post-migraciÃ³n)

Cache Hit Rate:
Target: > 80%
Current: (se mide post-migraciÃ³n)

Deploy Frequency:
Target: Daily deploys sin downtime
Current: (se mide post-migraciÃ³n)

MTTR (Mean Time To Recovery):
Target: < 30 minutos
Current: (se mide post-migraciÃ³n)
```

**MÃ©tricas de negocio**:
```
User Satisfaction:
Target: > 4.5/5 en encuestas
MÃ©todo: NPS surveys post-migraciÃ³n

Support Tickets:
Target: No incremento vs baseline
Baseline: X tickets/semana
Current: (se mide post-migraciÃ³n)

API Uptime (para terceros):
Target: > 99.95%
SLA commitment: 99.9%
Current: (se mide post-migraciÃ³n)

Feature Velocity:
Target: 2x mÃ¡s rÃ¡pido desarrollar features
MÃ©todo: Tiempo promedio de feature
Current: (se mide post-migraciÃ³n)
```

### 11.5 GestiÃ³n de Riesgos

| Riesgo | Probabilidad | Impacto | MitigaciÃ³n | Plan B |
|--------|--------------|---------|------------|--------|
| **PÃ©rdida de datos** | Media | CrÃ­tico | - Dual write mode<br>- ValidaciÃ³n automÃ¡tica<br>- DLQ para eventos fallidos | - Rollback inmediato<br>- Recuperar de backups<br>- Re-procesar eventos |
| **Performance degradado** | Media | Alto | - Load testing previo<br>- Traffic split gradual<br>- Auto-scaling configurado | - Rollback a sistema viejo<br>- Escalar recursos<br>- Optimizar queries |
| **Incompatibilidad de datos** | Baja | Alto | - Schema validation<br>- Tests con datos reales<br>- Dry-run migrations | - Rollback<br>- Ajustar mappers<br>- Re-migrar datos |
| **Downtime prolongado** | Baja | CrÃ­tico | - MigraciÃ³n sin downtime<br>- Health checks robustos<br>- Failover automÃ¡tico | - Activar DR site<br>- Comunicar a usuarios<br>- CompensaciÃ³n por SLA |
| **Bugs crÃ­ticos en producciÃ³n** | Media | Alto | - Extensive testing<br>- Staged rollout<br>- Feature flags | - Rollback inmediato<br>- Hotfix<br>- Post-mortem |
| **Equipo sin suficiente conocimiento** | Media | Medio | - Training antes de migraciÃ³n<br>- Pair programming<br>- DocumentaciÃ³n completa | - Contratar consultores<br>- Extended timeline<br>- Mentoring |
| **Costos AWS exceden presupuesto** | Media | Medio | - Cost monitoring<br>- Reserved instances<br>- Auto-scaling limits | - Ajustar recursos<br>- Optimizar uso<br>- Negociar con AWS |
| **Fallo en comunicaciÃ³n gRPC** | Baja | Alto | - Circuit breaker<br>- Retry logic<br>- Fallback a REST | - Usar REST temporalmente<br>- Debug gRPC<br>- Escalar timeout |

### 11.6 ComunicaciÃ³n con Stakeholders

**Pre-MigraciÃ³n** (1 semana antes):
```
Email a stakeholders:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Subject: MigraciÃ³n Backend HSE - Inicio [Fecha]

Resumen:
- QuÃ© se migrarÃ¡
- CuÃ¡ndo comenzarÃ¡
- Impacto esperado (ninguno/mÃ­nimo)
- Ventanas de mantenimiento (si aplica)
- Contacto de emergencia
```

**Durante MigraciÃ³n** (updates diarios):
```
Slack channel: #migration-updates

Daily update format:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MigraciÃ³n Backend - DÃ­a X       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Estado: âœ… On track             â”‚
â”‚ Traffic split: 50%              â”‚
â”‚ Error rate: 0.05% (âœ… target)  â”‚
â”‚ Issues: Ninguno                 â”‚
â”‚ PrÃ³ximos pasos: Incremento 75% â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Post-MigraciÃ³n** (1 semana despuÃ©s):
```
Email de cierre:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Subject: âœ… MigraciÃ³n Backend HSE - Completada

Resumen:
- Estado final: Exitosa
- MÃ©tricas alcanzadas
- Mejoras obtenidas
- PrÃ³ximos pasos
- Agradecimientos al equipo
```

---

**ğŸ“„ ContinÃºa en**: `12-13_Monitoreo_Ejecucion.md`
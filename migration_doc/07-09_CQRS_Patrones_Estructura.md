# Plan de Migraci√≥n HSE Backend - Secciones 7-9

**Continuaci√≥n desde**: `04-06_Modulos_Datos_Reglas.md`

---

## 7. Arquitectura CQRS en Devices Service

### 7.1 Visi√≥n General de CQRS

**CQRS** (Command Query Responsibility Segregation) separa las operaciones de escritura (commands) de las operaciones de lectura (queries).

**Por qu√© lo necesitamos**:
```
Problema actual:
- 100+ eventos/segundo que debemos GUARDAR (writes)
- Usuarios consultando dashboards y gr√°ficas (reads)
- Ambos compiten por los mismos recursos

Con CQRS:
WRITE SIDE                          READ SIDE
- Optimizado para velocidad         - Optimizado para consultas
- Queue + batch processing          - Cache + agregaciones pre-calculadas
- Latencia < 10ms                   - Queries complejas permitidas
- Sin bloqueos de lectura           - Sin impacto de escrituras
```

### 7.2 Arquitectura Completa

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  WRITE SIDE (Commands)                       ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Eventos entrantes                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇTCP Server  ‚îÇ   ‚îÇ WS Server  ‚îÇ   ‚îÇgRPC Server ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ(Port 5000) ‚îÇ   ‚îÇ(Port 5001) ‚îÇ   ‚îÇ(Port 50051)‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ         ‚îÇ                ‚îÇ                ‚îÇ                 ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ                          ‚ñº                                   ‚îÇ
‚îÇ                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
‚îÇ                 ‚îÇ Event Validator ‚îÇ                         ‚îÇ
‚îÇ                 ‚îÇ (Schema check)  ‚îÇ                         ‚îÇ
‚îÇ                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
‚îÇ                          ‚îÇ                                   ‚îÇ
‚îÇ                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
‚îÇ                 ‚îÇ  Redis Queue    ‚îÇ ‚Üê LPUSH (< 1ms)        ‚îÇ
‚îÇ                 ‚îÇ  "events:queue" ‚îÇ                         ‚îÇ
‚îÇ                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
‚îÇ                          ‚îÇ                                   ‚îÇ
‚îÇ                  [ACK inmediato]                            ‚îÇ
‚îÇ                          ‚îÇ                                   ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ         ‚ñº                                  ‚ñº                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇRedis Cache   ‚îÇ                 ‚îÇEvent Processor‚îÇ         ‚îÇ
‚îÇ  ‚îÇ(Update state)‚îÇ                 ‚îÇ(Background)   ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                                           ‚îÇ                  ‚îÇ
‚îÇ                                           ‚ñº                  ‚îÇ
‚îÇ                                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ                                  ‚îÇ  TimescaleDB    ‚îÇ        ‚îÇ
‚îÇ                                  ‚îÇ (Batch inserts) ‚îÇ        ‚îÇ
‚îÇ                                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Latencia total: < 10ms (solo Redis)                       ‚îÇ
‚îÇ  Throughput: 1000+ eventos/seg                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   READ SIDE (Queries)                        ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                             ‚îÇ
‚îÇ  ‚îÇ   gRPC     ‚îÇ                                             ‚îÇ
‚îÇ  ‚îÇ  Request   ‚îÇ                                             ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                             ‚îÇ
‚îÇ         ‚îÇ                                                    ‚îÇ
‚îÇ         ‚ñº                                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                           ‚îÇ
‚îÇ  ‚îÇQuery Handler‚îÇ                                           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                           ‚îÇ
‚îÇ         ‚îÇ                                                    ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                           ‚îÇ
‚îÇ    ‚ñº           ‚ñº                                            ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ
‚îÇ ‚îÇ  Redis  ‚îÇ ‚îÇ   TimescaleDB    ‚îÇ                          ‚îÇ
‚îÇ ‚îÇ (L1)    ‚îÇ ‚îÇ                  ‚îÇ                          ‚îÇ
‚îÇ ‚îÇ         ‚îÇ ‚îÇ Raw events       ‚îÇ                          ‚îÇ
‚îÇ ‚îÇHot data ‚îÇ ‚îÇ +                ‚îÇ                          ‚îÇ
‚îÇ ‚îÇTTL: 1min‚îÇ ‚îÇ Continuous       ‚îÇ                          ‚îÇ
‚îÇ ‚îÇ         ‚îÇ ‚îÇ aggregates       ‚îÇ                          ‚îÇ
‚îÇ ‚îÇHit: 85% ‚îÇ ‚îÇ (pre-calculated) ‚îÇ                          ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  Query latency:                                            ‚îÇ
‚îÇ  - Cache hit: < 5ms                                        ‚îÇ
‚îÇ  - Cache miss: < 200ms                                     ‚îÇ
‚îÇ  - Aggregate queries: < 100ms                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 7.3 Write Side - Flujo Detallado

#### 7.3.1 Recepci√≥n de Eventos

```go
// internal/infrastructure/protocols/tcp/tcp_handler.go
func (h *TCPHandler) HandleConnection(conn net.Conn) {
    defer conn.Close()
    
    scanner := bufio.NewScanner(conn)
    for scanner.Scan() {
        rawData := scanner.Bytes()
        
        // Parsear evento
        event, err := h.parseEvent(rawData)
        if err != nil {
            log.Error().Err(err).Msg("Invalid event format")
            continue
        }
        
        // Validaci√≥n b√°sica
        if err := h.validator.Validate(event); err != nil {
            log.Error().Err(err).Msg("Validation failed")
            continue
        }
        
        // Encolar en Redis (< 1ms)
        if err := h.eventQueue.Enqueue(event); err != nil {
            log.Error().Err(err).Msg("Failed to enqueue")
            continue
        }
        
        // ACK inmediato al dispositivo
        conn.Write([]byte("ACK\n"))
    }
}
```

**Por qu√© ACK inmediato**:
- Dispositivo no espera a que guardemos en DB
- Sin timeouts en dispositivos
- Sin re-env√≠os innecesarios
- Redis es confiable (persistencia en disco)

#### 7.3.2 Event Queue en Redis

```go
// internal/infrastructure/persistence/redis/event_queue.go
type EventQueue struct {
    client *redis.Client
}

func (eq *EventQueue) Enqueue(event *domain.DeviceEvent) error {
    eventJSON, err := json.Marshal(event)
    if err != nil {
        return err
    }
    
    // LPUSH a la lista
    return eq.client.LPush(
        context.Background(),
        "events:queue",
        eventJSON,
    ).Err()
}

func (eq *EventQueue) DequeueMultiple(count int64) ([]*domain.DeviceEvent, error) {
    // RPOP m√∫ltiples elementos (batch)
    results, err := eq.client.RPopCount(
        context.Background(),
        "events:queue",
        count,
    ).Result()
    
    if err != nil {
        return nil, err
    }
    
    events := make([]*domain.DeviceEvent, 0, len(results))
    for _, result := range results {
        var event domain.DeviceEvent
        if err := json.Unmarshal([]byte(result), &event); err != nil {
            continue
        }
        events = append(events, &event)
    }
    
    return events, nil
}
```

#### 7.3.3 Batch Processor

```go
// internal/application/services/event_processor.go
func (ep *EventProcessor) Start(ctx context.Context) error {
    ticker := time.NewTicker(ep.flushInterval) // 5 segundos
    defer ticker.Stop()
    
    batch := make([]*domain.DeviceEvent, 0, ep.batchSize)
    
    for {
        select {
        case <-ctx.Done():
            // Flush final antes de cerrar
            if len(batch) > 0 {
                ep.flushBatch(ctx, batch)
            }
            return ctx.Err()
            
        case <-ticker.C:
            // Flush peri√≥dico cada 5 segundos
            if len(batch) > 0 {
                ep.flushBatch(ctx, batch)
                batch = batch[:0]
            }
            
        default:
            // Intentar leer eventos (max 100 por vez)
            events, err := ep.eventQueue.DequeueMultiple(100)
            if err != nil || len(events) == 0 {
                time.Sleep(100 * time.Millisecond)
                continue
            }
            
            batch = append(batch, events...)
            
            // Flush si batch est√° lleno
            if len(batch) >= ep.batchSize {
                ep.flushBatch(ctx, batch)
                batch = batch[:0]
            }
        }
    }
}

func (ep *EventProcessor) flushBatch(
    ctx context.Context,
    events []*domain.DeviceEvent,
) error {
    start := time.Now()
    
    // 1. Batch insert a TimescaleDB
    if err := ep.eventWriter.InsertBatch(ctx, events); err != nil {
        log.Error().Err(err).Msg("Batch insert failed")
        ep.dlqManager.SaveFailedEvents(ctx, events, err.Error())
        return err
    }
    
    // 2. Actualizar cache (estados actuales)
    ep.cacheManager.UpdateDeviceStates(ctx, events)
    
    // 3. Notificar eventos relevantes al monolito
    relevantEvents := ep.filterRelevantEvents(events)
    if len(relevantEvents) > 0 {
        ep.grpcNotifier.NotifyEvents(ctx, relevantEvents)
    }
    
    duration := time.Since(start)
    log.Info().
        Int("count", len(events)).
        Dur("duration", duration).
        Msg("Batch processed")
    
    return nil
}
```

**Ventajas del batch processing**:
```
Sin batching (1 insert por evento):
- 100 eventos = 100 queries a DB
- Overhead de red: 100 √ó 5ms = 500ms
- Tiempo total: ~1 segundo

Con batching (1 insert para 100 eventos):
- 100 eventos = 1 query a DB
- Overhead de red: 1 √ó 5ms = 5ms
- Tiempo total: ~50ms

Mejora: 20x m√°s r√°pido
```

### 7.4 Read Side - Optimizaci√≥n de Consultas

#### 7.4.1 Cach√© en Capas

```go
// internal/application/queries/handlers/get_device_status.go
func (h *GetDeviceStatusHandler) Handle(
    ctx context.Context,
    deviceID uuid.UUID,
) (*DeviceStatus, error) {
    // Nivel 1: Redis cache (estados actuales)
    cached, err := h.cacheManager.GetDeviceState(ctx, deviceID)
    if err == nil {
        return cached, nil // Hit: < 5ms
    }
    
    // Nivel 2: TimescaleDB (√∫ltimo evento)
    lastEvent, err := h.eventReader.GetLatestEvent(ctx, deviceID)
    if err != nil {
        return nil, err
    }
    
    // Construir estado y cachear
    status := &DeviceStatus{
        DeviceID:  deviceID,
        Status:    determineStatus(lastEvent),
        LastEvent: lastEvent,
        UpdatedAt: time.Now(),
    }
    
    h.cacheManager.SetDeviceState(ctx, status)
    
    return status, nil
}
```

#### 7.4.2 TimescaleDB - Continuous Aggregates

**Problema**: Gr√°fica de consumo promedio por hora de los √∫ltimos 30 d√≠as.

**Sin continuous aggregates**:
```sql
-- Escanea millones de rows
SELECT 
  date_trunc('hour', time) as hour,
  AVG(value) as avg_consumption
FROM device_events
WHERE device_id = 'abc-123'
  AND time >= NOW() - INTERVAL '30 days'
  AND event_type = 'energy_consumption'
GROUP BY hour
ORDER BY hour;

-- Tiempo: 10-30 segundos (depende de data)
```

**Con continuous aggregates**:
```sql
-- 1. Crear vista materializada (una vez)
CREATE MATERIALIZED VIEW device_events_hourly
WITH (timescaledb.continuous) AS
SELECT 
  time_bucket('1 hour', time) AS hour,
  device_id,
  event_type,
  AVG(value) AS avg_value,
  MAX(value) AS max_value,
  MIN(value) AS min_value,
  COUNT(*) AS event_count
FROM device_events
GROUP BY hour, device_id, event_type;

-- 2. Pol√≠tica de refresh autom√°tico
SELECT add_continuous_aggregate_policy(
  'device_events_hourly',
  start_offset => INTERVAL '3 hours',
  end_offset => INTERVAL '1 hour',
  schedule_interval => INTERVAL '10 minutes'
);

-- 3. Query usa la vista (instant√°neo)
SELECT hour, avg_value
FROM device_events_hourly
WHERE device_id = 'abc-123'
  AND event_type = 'energy_consumption'
  AND hour >= NOW() - INTERVAL '30 days'
ORDER BY hour;

-- Tiempo: < 100ms
```

**Agregaciones disponibles**:
```sql
-- Por hora (para gr√°ficas de hoy/ayer)
device_events_hourly

-- Por d√≠a (para gr√°ficas de semana/mes)
device_events_daily

-- Por semana (para gr√°ficas de a√±o)
device_events_weekly
```

### 7.5 Consistencia Eventual

**Trade-off aceptado**:
```
Evento ocurre en dispositivo ‚Üí Llega a Devices Service ‚Üí Se guarda en queue
                                                              ‚Üì
                                         [1-5 segundos de delay]
                                                              ‚Üì
                                     Batch processor ‚Üí TimescaleDB ‚Üí Cache actualizado
                                                              ‚Üì
                                                    Usuario ve el evento

Delay total: 1-5 segundos
```

**Por qu√© es aceptable**:
- Dashboards no requieren actualizaci√≥n instant√°nea
- Usuarios entienden "datos casi en tiempo real"
- Para acciones cr√≠ticas (alarmas), usamos notificaciones directas v√≠a gRPC

**Para eventos cr√≠ticos** (bypass del delay):
```go
func (ep *EventProcessor) processEvent(event *domain.DeviceEvent) {
    // Eventos cr√≠ticos se notifican inmediatamente
    if event.Type == "alarm" || event.Type == "error" {
        ep.grpcNotifier.NotifyImmediately(event) // < 100ms
    }
    
    // Todos los eventos van a queue normal
    ep.eventQueue.Enqueue(event)
}
```

---

## 8. Patrones Cr√≠ticos de Implementaci√≥n

### 8.1 Circuit Breaker

**Problema**: Si el monolito cae, el Devices Service no debe colapsar intentando notificarlo.

**Soluci√≥n**: Circuit Breaker que detecta fallos y "abre" el circuito.

```go
// internal/infrastructure/grpc/client/monolith_client.go
import "github.com/sony/gobreaker"

type MonolithClient struct {
    client  pb.DeviceStreamClient
    breaker *gobreaker.CircuitBreaker
}

func NewMonolithClient(conn *grpc.ClientConn) *MonolithClient {
    settings := gobreaker.Settings{
        Name:        "monolith-grpc",
        MaxRequests: 3,              // requests en half-open
        Interval:    10 * time.Second,
        Timeout:     30 * time.Second,
        ReadyToTrip: func(counts gobreaker.Counts) bool {
            // Abre si 60% de requests fallan
            failureRatio := float64(counts.TotalFailures) / float64(counts.Requests)
            return counts.Requests >= 3 && failureRatio >= 0.6
        },
        OnStateChange: func(name string, from, to gobreaker.State) {
            log.Warn().
                Str("from", from.String()).
                Str("to", to.String()).
                Msg("Circuit breaker state changed")
        },
    }
    
    return &MonolithClient{
        client:  pb.NewDeviceStreamClient(conn),
        breaker: gobreaker.NewCircuitBreaker(settings),
    }
}

func (mc *MonolithClient) NotifyEvents(
    ctx context.Context,
    events []*domain.DeviceEvent,
) error {
    _, err := mc.breaker.Execute(func() (interface{}, error) {
        return nil, mc.sendEventsToMonolith(ctx, events)
    })
    
    if err == gobreaker.ErrOpenState {
        // Circuito abierto, guardar en DLQ
        log.Warn().Msg("Circuit open, saving to DLQ")
        return mc.dlqManager.SaveFailedEvents(events)
    }
    
    return err
}
```

**Estados del Circuit Breaker**:
```
CLOSED (normal)
  ‚Üì (m√∫ltiples fallos)
OPEN (no intenta llamadas)
  ‚Üì (despu√©s de timeout)
HALF-OPEN (permite algunas llamadas de prueba)
  ‚Üì
  ‚îú‚îÄ Si funcionan ‚Üí CLOSED
  ‚îî‚îÄ Si fallan ‚Üí OPEN
```

### 8.2 Dead Letter Queue (DLQ)

**Problema**: Si el batch insert falla, no podemos perder esos eventos.

**Soluci√≥n**: Guardar eventos fallidos en DLQ para retry posterior.

```go
// internal/application/services/dlq_manager.go
type DLQItem struct {
    Event     *domain.DeviceEvent
    Reason    string
    Timestamp time.Time
    Retries   int
}

func (dlq *DLQManager) SaveFailedEvents(
    ctx context.Context,
    events []*domain.DeviceEvent,
    reason string,
) error {
    for _, event := range events {
        item := DLQItem{
            Event:     event,
            Reason:    reason,
            Timestamp: time.Now(),
            Retries:   0,
        }
        
        itemJSON, _ := json.Marshal(item)
        dlq.redisClient.LPush(ctx, "events:dlq", itemJSON)
    }
    
    return nil
}

// Processor de DLQ (retry con exponential backoff)
func (dlq *DLQManager) StartDLQProcessor(ctx context.Context) {
    ticker := time.NewTicker(30 * time.Second)
    defer ticker.Stop()
    
    for {
        select {
        case <-ctx.Done():
            return
            
        case <-ticker.C:
            dlq.processDLQ(ctx)
        }
    }
}

func (dlq *DLQManager) processDLQ(ctx context.Context) {
    items, _ := dlq.redisClient.LRange(ctx, "events:dlq", 0, 99).Result()
    
    for _, itemJSON := range items {
        var item DLQItem
        json.Unmarshal([]byte(itemJSON), &item)
        
        // Exponential backoff
        backoff := time.Duration(math.Pow(2, float64(item.Retries))) * time.Second
        if time.Since(item.Timestamp) < backoff {
            continue
        }
        
        // Reintentar
        err := dlq.eventWriter.InsertBatch(ctx, []*domain.DeviceEvent{item.Event})
        if err != nil {
            item.Retries++
            item.Timestamp = time.Now()
            
            if item.Retries >= 5 {
                // Mover a DLQ permanente (requiere intervenci√≥n manual)
                dlq.moveToPermDLQ(ctx, item)
                dlq.redisClient.LRem(ctx, "events:dlq", 1, itemJSON)
            } else {
                // Actualizar retry count
                updatedJSON, _ := json.Marshal(item)
                dlq.redisClient.LRem(ctx, "events:dlq", 1, itemJSON)
                dlq.redisClient.LPush(ctx, "events:dlq", updatedJSON)
            }
        } else {
            // √âxito, remover de DLQ
            dlq.redisClient.LRem(ctx, "events:dlq", 1, itemJSON)
        }
    }
}
```

**Backoff exponencial**:
```
Retry 1: 1 segundo despu√©s
Retry 2: 2 segundos despu√©s
Retry 3: 4 segundos despu√©s
Retry 4: 8 segundos despu√©s
Retry 5: 16 segundos despu√©s
‚Üí Si falla 5 veces: DLQ permanente (alerta al equipo)
```

### 8.3 gRPC Streaming Bidireccional

#### 8.3.1 Protocol Buffers Definition

```protobuf
// internal/infrastructure/grpc/proto/devices.proto
syntax = "proto3";
package devices;

service DeviceStream {
  // Monolito se suscribe a cambios de dispositivos
  rpc SubscribeToDeviceChanges(SubscriptionRequest) 
    returns (stream DeviceChangeEvent);
  
  // Monolito env√≠a comandos
  rpc SendDeviceCommand(DeviceCommand) 
    returns (CommandResponse);
  
  // Queries de datos
  rpc GetDeviceStatus(DeviceStatusRequest)
    returns (DeviceStatusResponse);
}

message SubscriptionRequest {
  repeated string device_ids = 1;
  repeated string event_types = 2;  // 'alarm', 'error', 'state_change'
  string client_id = 3;
}

message DeviceChangeEvent {
  string device_id = 1;
  string event_type = 2;
  int64 timestamp = 3;
  double value = 4;
  string unit = 5;
  map<string, string> metadata = 6;
}

message DeviceCommand {
  string device_id = 1;
  string command_type = 2;
  map<string, string> params = 3;
  string request_id = 4;
}

message CommandResponse {
  bool success = 1;
  string message = 2;
  string request_id = 3;
}
```

#### 8.3.2 Server Implementation (Go)

```go
// internal/infrastructure/grpc/server/devices_server.go
func (s *DeviceServer) SubscribeToDeviceChanges(
    req *pb.SubscriptionRequest,
    stream pb.DeviceStream_SubscribeToDeviceChangesServer,
) error {
    ctx := stream.Context()
    
    // Canal para este cliente
    clientChan := make(chan *domain.DeviceEvent, 100)
    clientID := req.ClientId
    
    // Registrar subscriber
    s.connectionManager.RegisterSubscriber(
        clientID,
        clientChan,
        req.DeviceIds,
        req.EventTypes,
    )
    defer s.connectionManager.UnregisterSubscriber(clientID)
    
    log.Info().Str("client_id", clientID).Msg("Client subscribed")
    
    // Stream eventos
    for {
        select {
        case event := <-clientChan:
            pbEvent := &pb.DeviceChangeEvent{
                DeviceId:  event.DeviceID.String(),
                EventType: event.Type,
                Timestamp: event.Time.Unix(),
                Value:     event.Value,
                Unit:      event.Unit,
                Metadata:  event.Metadata,
            }
            
            if err := stream.Send(pbEvent); err != nil {
                return err
            }
            
        case <-ctx.Done():
            log.Info().Str("client_id", clientID).Msg("Client disconnected")
            return nil
        }
    }
}
```

#### 8.3.3 Client Implementation (TypeScript)

```typescript
// src/shared/infrastructure/grpc/devices-grpc.client.ts
@Injectable()
export class DevicesGrpcClient implements OnModuleInit {
  private client: any;
  private deviceChanges$ = new Subject<DeviceChangeEvent>();
  private stream: any;

  async onModuleInit() {
    const packageDef = protoLoader.loadSync('./proto/devices.proto');
    const proto = grpc.loadPackageDefinition(packageDef);
    
    this.client = new proto.devices.DeviceStream(
      process.env.DEVICES_GRPC_URL,
      grpc.credentials.createInsecure(),
    );
    
    await this.subscribeToDeviceChanges();
  }

  private async subscribeToDeviceChanges() {
    const request = {
      device_ids: [],  // vac√≠o = todos
      event_types: ['alarm', 'state_change', 'error'],
      client_id: `monolith-${process.env.INSTANCE_ID}`,
    };

    this.stream = this.client.SubscribeToDeviceChanges(request);

    this.stream.on('data', (event: any) => {
      this.deviceChanges$.next({
        deviceId: event.device_id,
        eventType: event.event_type,
        timestamp: new Date(event.timestamp * 1000),
        value: event.value,
        unit: event.unit,
      });
    });

    this.stream.on('error', (error: any) => {
      console.error('gRPC stream error:', error);
      setTimeout(() => this.subscribeToDeviceChanges(), 5000);
    });

    this.stream.on('end', () => {
      console.log('gRPC stream ended, reconnecting...');
      setTimeout(() => this.subscribeToDeviceChanges(), 1000);
    });
  }

  // Observable para m√≥dulos que necesiten escuchar cambios
  onDeviceChange(): Observable<DeviceChangeEvent> {
    return this.deviceChanges$.asObservable();
  }

  // Enviar comando
  async sendCommand(
    deviceId: string,
    commandType: string,
    params: Record<string, string>,
  ): Promise<CommandResponse> {
    return new Promise((resolve, reject) => {
      this.client.SendDeviceCommand(
        {
          device_id: deviceId,
          command_type: commandType,
          params: params,
          request_id: uuid(),
        },
        (error: any, response: any) => {
          if (error) reject(error);
          else resolve(response);
        },
      );
    });
  }
}
```

### 8.4 Health Checks Robustos

```go
// cmd/server/main.go
func setupHealthChecks(e *echo.Echo, deps *Dependencies) {
    // Liveness: Solo verifica que el proceso est√© vivo
    e.GET("/health/liveness", func(c echo.Context) error {
        return c.JSON(200, map[string]string{
            "status": "alive",
        })
    })
    
    // Readiness: Verifica que todas las dependencias est√©n listas
    e.GET("/health/readiness", func(c echo.Context) error {
        checks := make(map[string]bool)
        
        // Check PostgreSQL
        ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)
        defer cancel()
        err := deps.DB.PingContext(ctx)
        checks["postgres"] = (err == nil)
        
        // Check Redis
        err = deps.Redis.Ping(ctx).Err()
        checks["redis"] = (err == nil)
        
        // Check gRPC server
        checks["grpc_server"] = deps.GRPCServer.IsRunning()
        
        // Check TCP server
        checks["tcp_server"] = deps.TCPServer.IsRunning()
        
        // Determinar si est√° ready
        allHealthy := true
        for _, healthy := range checks {
            if !healthy {
                allHealthy = false
                break
            }
        }
        
        status := 200
        if !allHealthy {
            status = 503
        }
        
        return c.JSON(status, map[string]interface{}{
            "status": allHealthy,
            "checks": checks,
        })
    })
}
```

**Por qu√© dos endpoints**:
- **Liveness**: Kubernetes usa esto para saber si debe reiniciar el pod
- **Readiness**: Kubernetes usa esto para saber si debe enviar tr√°fico

---

## 9. Estructura de Carpetas

### 9.1 Monolito Modular (NestJS)

```
hse-backend-monolith/
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.ts
‚îÇ   ‚îú‚îÄ‚îÄ app.module.ts
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ shared/                          # Compartido entre m√≥dulos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ infrastructure/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prisma/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prisma.service.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema.prisma
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ migrations/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ database.module.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cache/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ redis.service.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cache.module.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ grpc/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ proto/devices.proto
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ devices-grpc.client.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ grpc.module.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ monitoring/
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ newrelic.service.ts
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ monitoring.module.ts
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ domain/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ value-objects/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ email.vo.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ uuid.vo.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ date-range.vo.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ exceptions/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ domain.exception.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validation.exception.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ not-found.exception.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ interfaces/
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ repository.interface.ts
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ application/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ decorators/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ current-user.decorator.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ roles.decorator.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ guards/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ jwt-auth.guard.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ permissions.guard.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api-key.guard.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ interceptors/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logging.interceptor.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transform.interceptor.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipes/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validation.pipe.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ filters/
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ all-exceptions.filter.ts
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ date.utils.ts
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ hash.utils.ts
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ pagination.utils.ts
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ modules/                         # M√≥dulos de negocio (DDD)
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth/                        # M√≥dulo de autenticaci√≥n
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth.module.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ domain/                  # Capa de dominio
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entities/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user.entity.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api-key.entity.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ value-objects/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ password.vo.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repositories/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user.repository.interface.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api-key.repository.interface.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ password.service.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ application/             # Casos de uso
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ use-cases/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ login/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ login.use-case.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ login.dto.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ register/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ register.use-case.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ register.dto.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ refresh-token/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ refresh-token.use-case.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validate-api-key/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ validate-api-key.use-case.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ jwt.service.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ infrastructure/          # Adaptadores
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ persistence/
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ prisma/
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user.repository.ts
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api-key.repository.ts
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ mappers/
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ user.mapper.ts
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ api-key.mapper.ts
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ http/
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ graphql/
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ resolvers/
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ auth.resolver.ts
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ types/
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ auth.types.ts
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ rest/
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ controllers/
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ           ‚îî‚îÄ‚îÄ auth.controller.ts
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ strategies/
‚îÇ   ‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ jwt.strategy.ts
‚îÇ   ‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ api-key.strategy.ts
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ iot-core/                    # M√≥dulo IoT/Core
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ iot-core.module.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ domain/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entities/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ organization.entity.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ establishment.entity.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ area.entity.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ device.entity.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ role.entity.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ action.entity.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ permission.entity.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ value-objects/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ device-status.vo.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ location.vo.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repositories/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ organization.repository.interface.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ establishment.repository.interface.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ device.repository.interface.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ role.repository.interface.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ permissions.service.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ device-commands.service.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ application/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ use-cases/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ organizations/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ create-organization.use-case.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ list-organizations.use-case.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ *.dto.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ establishments/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ create-establishment.use-case.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ assign-user.use-case.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ *.dto.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ devices/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ control-device.use-case.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ get-device-status.use-case.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ *.dto.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ roles/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ create-role.use-case.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ assign-permissions.use-case.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ *.dto.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ energy-analytics.service.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ infrastructure/
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ persistence/
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ prisma/
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ organization.repository.ts
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ establishment.repository.ts
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ device.repository.ts
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ role.repository.ts
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ mappers/
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ http/
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ graphql/
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ resolvers/
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ organization.resolver.ts
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ establishment.resolver.ts
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ device.resolver.ts
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ role.resolver.ts
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ types/
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ rest/
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ controllers/
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ grpc/
‚îÇ   ‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ device-commands.client.ts
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ booking/                     # M√≥dulo de reservas
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ booking.module.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ domain/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entities/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ guest.entity.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ booking.entity.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ notification.entity.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ doorlock-code.entity.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ value-objects/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ booking-status.vo.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ date-range.vo.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ doorlock-code.vo.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repositories/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ guest.repository.interface.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ booking.repository.interface.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ booking-validation.service.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ doorlock-code-generator.service.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ notification.service.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ application/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ use-cases/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ create-booking.use-case.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ cancel-booking.use-case.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ confirm-booking.use-case.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ check-availability.use-case.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ *.dto.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ infrastructure/
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ persistence/
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ prisma/
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ guest.repository.ts
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ booking.repository.ts
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ http/
‚îÇ   ‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ graphql/
‚îÇ   ‚îÇ   ‚îÇ               ‚îî‚îÄ‚îÄ resolvers/
‚îÇ   ‚îÇ   ‚îÇ                   ‚îî‚îÄ‚îÄ booking.resolver.ts
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ smart-access/                # A desarrollar
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ smart-access.module.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ domain/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ application/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ infrastructure/
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ universities/                # A desarrollar
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ universities.module.ts
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ domain/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ application/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ infrastructure/
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ config/
‚îÇ       ‚îú‚îÄ‚îÄ app.config.ts
‚îÇ       ‚îú‚îÄ‚îÄ database.config.ts
‚îÇ       ‚îú‚îÄ‚îÄ redis.config.ts
‚îÇ       ‚îú‚îÄ‚îÄ jwt.config.ts
‚îÇ       ‚îî‚îÄ‚îÄ graphql.config.ts
‚îÇ
‚îú‚îÄ‚îÄ test/
‚îÇ   ‚îú‚îÄ‚îÄ unit/                            # Tests unitarios
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ iot-core/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ booking/
‚îÇ   ‚îú‚îÄ‚îÄ integration/                     # Tests de integraci√≥n
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ *.spec.ts
‚îÇ   ‚îî‚îÄ‚îÄ e2e/                             # Tests end-to-end
‚îÇ       ‚îî‚îÄ‚îÄ *.e2e-spec.ts
‚îÇ
‚îú‚îÄ‚îÄ prisma/
‚îÇ   ‚îú‚îÄ‚îÄ schema.prisma
‚îÇ   ‚îú‚îÄ‚îÄ migrations/
‚îÇ   ‚îî‚îÄ‚îÄ seed.ts
‚îÇ
‚îú‚îÄ‚îÄ proto/                               # Protocol Buffers
‚îÇ   ‚îî‚îÄ‚îÄ devices.proto
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ generate-proto.sh
‚îÇ   ‚îî‚îÄ‚îÄ db-migrate.sh
‚îÇ
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ .env.development
‚îú‚îÄ‚îÄ .env.production
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ tsconfig.json
‚îú‚îÄ‚îÄ nest-cli.json
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îî‚îÄ‚îÄ README.md
```

**Explicaci√≥n de la estructura**:

**`shared/`**: C√≥digo reutilizable entre m√≥dulos
- `infrastructure/`: Servicios de infraestructura (DB, cache, gRPC)
- `domain/`: Value objects y excepciones comunes
- `application/`: Guards, interceptors, pipes globales
- `utils/`: Funciones utilitarias

**`modules/`**: Cada m√≥dulo sigue arquitectura hexagonal
- `domain/`: L√≥gica de negocio pura (sin dependencias)
- `application/`: Casos de uso (orquesta el dominio)
- `infrastructure/`: Adaptadores (DB, HTTP, gRPC)

**Por qu√© esta organizaci√≥n**:
- **Separaci√≥n por feature**: Cada m√≥dulo es autocontenido
- **F√°cil de testear**: Domain layer sin dependencias
- **Escalable**: M√≥dulos pueden extraerse a microservicios
- **Mantenible**: Cambios en un m√≥dulo no afectan otros

### 9.2 Devices Service (Golang)

```
hse-devices-service/
‚îÇ
‚îú‚îÄ‚îÄ cmd/
‚îÇ   ‚îî‚îÄ‚îÄ server/
‚îÇ       ‚îî‚îÄ‚îÄ main.go                      # Entry point
‚îÇ
‚îú‚îÄ‚îÄ internal/                            # C√≥digo privado
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ domain/                          # Capa de dominio
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entities/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ device.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ device_event.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ device_state.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ connection.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ command.go
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ valueobjects/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ device_status.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ connection_state.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ event_type.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ protocol_type.go
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ repositories/                # Interfaces (puertos)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ device_repository.go
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ event_repository.go
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ query_repository.go
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ application/                     # Capa de aplicaci√≥n
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commands/                    # WRITE SIDE
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ handlers/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ save_device_event.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ send_device_command.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ update_device_status.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dto/
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ command_dto.go
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ queries/                     # READ SIDE
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ handlers/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ get_device_status.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ get_device_events.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ get_device_analytics.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ get_device_graph_data.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dto/
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ query_dto.go
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ connection_manager.go    # Pool de conexiones
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ event_queue.go           # Cola Redis
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ event_processor.go       # Batch processor
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ cache_manager.go         # Gesti√≥n de cache
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ dlq_manager.go           # Dead Letter Queue
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ infrastructure/                  # Capa de infraestructura
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ persistence/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postgres/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ timescale/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ event_writer.go      # Batch writes
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ event_reader.go      # Queries
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ aggregates.go        # Continuous aggregates
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ device_repository.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gorm_config.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ redis/
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ event_queue.go           # Cola de eventos
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ device_cache.go          # Cache de estados
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ connection_state.go      # Estados de conexi√≥n
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ protocols/                   # Adaptadores de protocolos
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ factory.go               # Abstract Factory
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tcp/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tcp_handler.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tcp_server.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tcp_client.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ websocket/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ws_handler.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ws_server.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ modbus/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ modbus_handler.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mqtt/
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ mqtt_handler.go
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ http/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ handlers/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ device_handler.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ command_handler.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ health_handler.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ middleware/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logging.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cors.go
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ router.go
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ grpc/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ proto/
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ devices.proto
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ commands.proto
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ server/
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ devices_server.go
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ commands_server.go
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ stream_manager.go
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ client/
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ monolith_client.go   # Con circuit breaker
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ pb/                       # C√≥digo generado
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ *.pb.go
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ config/
‚îÇ       ‚îú‚îÄ‚îÄ config.go
‚îÇ       ‚îî‚îÄ‚îÄ env.go
‚îÇ
‚îú‚îÄ‚îÄ pkg/                                 # C√≥digo p√∫blico/reutilizable
‚îÇ   ‚îú‚îÄ‚îÄ logger/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.go
‚îÇ   ‚îú‚îÄ‚îÄ errors/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ errors.go
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ retry.go
‚îÇ       ‚îî‚îÄ‚îÄ backoff.go
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ generate_proto.sh
‚îÇ   ‚îú‚îÄ‚îÄ migrate.sh
‚îÇ   ‚îî‚îÄ‚îÄ seed_timescale.sh
‚îÇ
‚îú‚îÄ‚îÄ migrations/
‚îÇ   ‚îú‚îÄ‚îÄ 001_initial_schema.sql
‚îÇ   ‚îú‚îÄ‚îÄ 002_timescale_setup.sql
‚îÇ   ‚îî‚îÄ‚îÄ 003_continuous_aggregates.sql
‚îÇ
‚îú‚îÄ‚îÄ test/
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ domain/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ application/
‚îÇ   ‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ *_test.go
‚îÇ   ‚îî‚îÄ‚îÄ mocks/
‚îÇ       ‚îî‚îÄ‚îÄ *.go
‚îÇ
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ .env.development
‚îú‚îÄ‚îÄ .env.production
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ go.mod
‚îú‚îÄ‚îÄ go.sum
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îî‚îÄ‚îÄ README.md
```

**Explicaci√≥n de la estructura**:

**`internal/`**: C√≥digo privado del servicio
- **`domain/`**: Entidades y l√≥gica de negocio pura
- **`application/`**: Separaci√≥n CQRS (commands/queries) + services
- **`infrastructure/`**: Adaptadores (DB, protocols, HTTP, gRPC)

**`pkg/`**: C√≥digo que podr√≠a ser reutilizado por otros servicios
- Logger, utils, errors comunes

**Por qu√© esta organizaci√≥n**:
- **CQRS expl√≠cito**: Commands y queries claramente separados
- **Clean architecture**: Dependencias apuntan hacia adentro
- **Testeable**: Domain sin dependencias externas
- **Escalable**: F√°cil a√±adir nuevos protocolos o handlers

### 9.3 Migraciones de Base de Datos

**TimescaleDB Setup** (`migrations/002_timescale_setup.sql`):
```sql
-- Activar extensi√≥n TimescaleDB
CREATE EXTENSION IF NOT EXISTS timescaledb CASCADE;

-- Crear tabla de eventos
CREATE TABLE device_events (
  time          TIMESTAMPTZ NOT NULL,
  device_id     UUID NOT NULL,
  event_type    VARCHAR(50) NOT NULL,
  value         NUMERIC,
  unit          VARCHAR(20),
  metadata      JSONB,
  source        VARCHAR(50)
);

-- Convertir a hypertable
SELECT create_hypertable('device_events', 'time');

-- √çndices optimizados
CREATE INDEX idx_device_events_device_time 
  ON device_events (device_id, time DESC);

CREATE INDEX idx_device_events_type 
  ON device_events (event_type, time DESC);

-- Compresi√≥n autom√°tica (datos > 7 d√≠as)
ALTER TABLE device_events SET (
  timescaledb.compress,
  timescaledb.compress_segmentby = 'device_id'
);

SELECT add_compression_policy('device_events', INTERVAL '7 days');

-- Retenci√≥n autom√°tica (eliminar datos > 2 a√±os)
SELECT add_retention_policy('device_events', INTERVAL '2 years');
```

**Continuous Aggregates** (`migrations/003_continuous_aggregates.sql`):
```sql
-- Agregaci√≥n por hora
CREATE MATERIALIZED VIEW device_events_hourly
WITH (timescaledb.continuous) AS
SELECT 
  time_bucket('1 hour', time) AS hour,
  device_id,
  event_type,
  AVG(value) AS avg_value,
  MAX(value) AS max_value,
  MIN(value) AS min_value,
  COUNT(*) AS event_count,
  STDDEV(value) AS stddev_value
FROM device_events
GROUP BY hour, device_id, event_type;

-- Pol√≠tica de refresh (cada 10 minutos)
SELECT add_continuous_aggregate_policy(
  'device_events_hourly',
  start_offset => INTERVAL '3 hours',
  end_offset => INTERVAL '1 hour',
  schedule_interval => INTERVAL '10 minutes'
);

-- Agregaci√≥n por d√≠a
CREATE MATERIALIZED VIEW device_events_daily
WITH (timescaledb.continuous) AS
SELECT 
  time_bucket('1 day', time) AS day,
  device_id,
  event_type,
  AVG(value) AS avg_value,
  MAX(value) AS max_value,
  MIN(value) AS min_value,
  SUM(value) AS total_value,
  COUNT(*) AS event_count
FROM device_events
GROUP BY day, device_id, event_type;

-- Pol√≠tica de refresh (cada hora)
SELECT add_continuous_aggregate_policy(
  'device_events_daily',
  start_offset => INTERVAL '3 days',
  end_offset => INTERVAL '1 day',
  schedule_interval => INTERVAL '1 hour'
);
```

---

**üìÑ Contin√∫a en**: `10-11_Infraestructura_Migracion.md`
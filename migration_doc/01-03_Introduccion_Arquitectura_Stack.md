# Plan de Migraci√≥n HSE Backend - Secciones 1-3

**Versi√≥n**: 2.0  
**Fecha**: 2024-01-04  
**Estado**: ‚úÖ Aprobado para ejecuci√≥n

---

## Tabla de Contenidos General

1. **[Introducci√≥n y Contexto](#1-introducci√≥n-y-contexto)** ‚Üê En este documento
2. **[Arquitectura General](#2-arquitectura-general)** ‚Üê En este documento
3. **[Stack Tecnol√≥gico](#3-stack-tecnol√≥gico)** ‚Üê En este documento
4. M√≥dulos del Sistema ‚Üí Ver documento `04-06_Modulos_Datos_Reglas.md`
5. Modelo de Datos ‚Üí Ver documento `04-06_Modulos_Datos_Reglas.md`
6. Reglas de Negocio y Flujos ‚Üí Ver documento `04-06_Modulos_Datos_Reglas.md`
7. Arquitectura CQRS en Devices Service ‚Üí Ver documento `07-09_CQRS_Patrones_Estructura.md`
8. Patrones Cr√≠ticos de Implementaci√≥n ‚Üí Ver documento `07-09_CQRS_Patrones_Estructura.md`
9. Estructura de Carpetas ‚Üí Ver documento `07-09_CQRS_Patrones_Estructura.md`
10. Infraestructura Cloud ‚Üí Ver documento `10-11_Infraestructura_Migracion.md`
11. Estrategia de Migraci√≥n ‚Üí Ver documento `10-11_Infraestructura_Migracion.md`
12. Monitoreo y Observabilidad ‚Üí Ver documento `12-13_Monitoreo_Ejecucion.md`
13. Plan de Ejecuci√≥n ‚Üí Ver documento `12-13_Monitoreo_Ejecucion.md`

---

## 1. Introducci√≥n y Contexto

### 1.1 Objetivos de la Migraci√≥n

Este plan describe la migraci√≥n del backend HSE hacia una arquitectura moderna que permita:

- **Migrar el core a tecnolog√≠as m√°s robustas**: Transici√≥n de la arquitectura actual a un stack m√°s maduro y mantenible
- **Implementar arquitectura bien definida**: Separaci√≥n clara de responsabilidades con arquitectura hexagonal y CQRS
- **Facilitar escalabilidad**: Dise√±o que permite crecer sin cambios estructurales mayores
- **Mejorar mantenibilidad**: C√≥digo organizado, testeado y documentado
- **Garantizar alta disponibilidad**: Cero p√©rdida de datos durante deploys y actualizaciones

### 1.2 Contexto del Sistema

El sistema HSE maneja operaciones cr√≠ticas de IoT con requisitos espec√≠ficos:

**Volumen de Datos**:
- **100+ eventos por segundo** de controladores IoT en tiempo real
- Datos de series temporales para an√°lisis y generaci√≥n de gr√°ficas
- Almacenamiento hist√≥rico para auditor√≠a y reportes

**Funcionalidades Actuales**:
- Eficiencia energ√©tica y monitoreo en tiempo real
- Automatizaci√≥n de predios y espacios
- Control de acceso inteligente (puertas, veh√≠culos)
- Sistema de reservas con integraci√≥n IoT
- An√°lisis y reportes de consumo energ√©tico

**Desaf√≠os Actuales**:
- P√©rdida de datos durante deploys del backend
- Dificultad para escalar componentes independientemente
- Acoplamiento entre l√≥gica de negocio y manejo de conexiones
- Falta de separaci√≥n clara entre escrituras y lecturas

### 1.3 Decisiones Arquitect√≥nicas Principales

#### 1.3.1 Separaci√≥n de Servicios

**Decisi√≥n**: Dividir el sistema en dos servicios principales:

1. **Monolito Modular (NestJS)**: Contiene toda la l√≥gica de negocio
2. **Devices Service (Golang)**: Maneja conexiones y eventos de dispositivos IoT

**Justificaci√≥n**:

Esta separaci√≥n resuelve varios problemas cr√≠ticos:

- **Evita p√©rdida de datos durante deploys**: Cuando actualizamos l√≥gica de negocio en el monolito, el Devices Service sigue recibiendo y almacenando eventos sin interrupci√≥n
- **Permite actualizaciones independientes**: Podemos actualizar reglas de negocio sin afectar la recolecci√≥n de datos
- **Alta disponibilidad garantizada**: Si un servicio cae, el otro mantiene operaci√≥n
- **Escalabilidad independiente**: Podemos escalar el Devices Service (muchas conexiones) sin escalar el monolito (l√≥gica de negocio)

**Ejemplo del problema que resuelve**:
```
ANTES (Monolito √∫nico):
Deploy de nueva feature ‚Üí Reinicio del servidor ‚Üí 2-3 minutos sin recibir eventos
‚Üí P√©rdida de 12,000-18,000 eventos (100 eventos/seg √ó 120-180 segundos)

DESPU√âS (Servicios separados):
Deploy del monolito ‚Üí Devices Service sigue activo ‚Üí CERO eventos perdidos
```

#### 1.3.2 Golang para Devices Service

**Decisi√≥n**: Usar Golang en vez de Node.js para el servicio de dispositivos.

**Justificaci√≥n t√©cnica**:

**1. Alta concurrencia nativa**:
- Goroutines son mucho m√°s livianas que threads (2KB vs 2MB de memoria)
- Scheduler eficiente para manejar 100,000+ goroutines simult√°neas
- Channel-based communication para coordinaci√≥n sin locks

**2. Bajo consumo de memoria**:
- Cr√≠tico para mantener miles de conexiones TCP persistentes
- Garbage collector optimizado para latencia baja
- Mejor uso de CPU comparado con Node.js para operaciones concurrentes

**3. Performance para I/O bound operations**:
- Runtime optimizado para operaciones de red
- syscalls eficientes para TCP/WebSocket
- Sin overhead del event loop de Node.js

**4. Simplicidad de deploy**:
- Binario √∫nico compilado, sin runtime ni node_modules
- Cross-compilation f√°cil para diferentes plataformas
- Menor superficie de ataque en seguridad

**Contexto de decisi√≥n**:
Con 100+ eventos por segundo constantes, necesitamos un lenguaje que:
- Maneje concurrencia sin penalizar performance
- Mantenga latencia baja (< 10ms) para ACKs
- Use memoria eficientemente para miles de conexiones

Go cumple estos requisitos mejor que Node.js para este caso de uso espec√≠fico.

#### 1.3.3 CQRS en Devices Service

**Decisi√≥n**: Implementar patr√≥n CQRS (Command Query Responsibility Segregation) en el servicio de dispositivos.

**Justificaci√≥n**:

**Problema**: Tenemos dos patrones de acceso completamente diferentes:

1. **Escrituras (Commands)**:
   - Alta frecuencia: 100+ eventos/segundo
   - Operaciones simples: INSERT
   - Prioridad: Latencia ultra baja (< 10ms)
   - No pueden fallar (cada evento es valioso)

2. **Lecturas (Queries)**:
   - Frecuencia media: Usuarios consultando dashboards, gr√°ficas
   - Operaciones complejas: Agregaciones, JOINs, filtros temporales
   - Prioridad: Resultado correcto y completo
   - Pueden cachear resultados

**Soluci√≥n con CQRS**:

```
WRITE SIDE (optimizado para velocidad):
Evento ‚Üí Validaci√≥n m√≠nima ‚Üí Redis Queue ‚Üí ACK (< 10ms)
Background: Batch processor ‚Üí TimescaleDB

READ SIDE (optimizado para an√°lisis):
Query ‚Üí Redis Cache ‚Üí TimescaleDB (raw + aggregates)
```

**Beneficios**:
- **Escalabilidad independiente**: Podemos a√±adir m√°s workers para writes sin afectar reads
- **Optimizaci√≥n espec√≠fica**: Base de datos de escritura vs base de datos de lectura pueden tener √≠ndices diferentes
- **Consistencia eventual aceptable**: Los dashboards pueden tener 1-5 segundos de delay, no es cr√≠tico
- **Sin bloqueos**: Writes y reads no compiten por los mismos recursos

#### 1.3.4 gRPC Bidireccional

**Decisi√≥n**: Usar gRPC con streaming bidireccional para comunicaci√≥n entre servicios.

**Justificaci√≥n**:

**1. Performance superior**:
- Protocol Buffers (binario) vs JSON (texto): ~7-10x m√°s r√°pido
- HTTP/2 multiplexing: M√∫ltiples requests en una conexi√≥n
- Header compression: Reduce overhead

**2. Streaming bidireccional nativo**:
```
Caso de uso real:
Monolito necesita ser notificado cuando cambia estado de dispositivo
‚Üí Sin gRPC: Polling cada N segundos (ineficiente)
‚Üí Con gRPC: Stream abierto, notificaciones instant√°neas
```

**3. Contratos bien definidos**:
- Protocol Buffers define schema estricto
- Generaci√≥n autom√°tica de c√≥digo (tipos seguros)
- Versionado de APIs m√°s f√°cil

**4. Comunicaci√≥n eficiente**:
```
Ejemplo: 1000 notificaciones/minuto
REST: 1000 conexiones HTTP + parsing JSON = ~500ms overhead
gRPC: 1 stream + mensajes binarios = ~50ms overhead
```

**Por qu√© NO usamos REST**:
- REST es excelente para APIs p√∫blicas (terceros)
- Para comunicaci√≥n interna alta frecuencia, gRPC es superior
- No necesitamos que sea "human-readable" (es interno)

---

## 2. Arquitectura General

### 2.1 Vista de Alto Nivel

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         CLIENTES                             ‚îÇ
‚îÇ  (Web App, Mobile App, APIs de Terceros)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚îÇ HTTP/HTTPS
                     ‚îÇ GraphQL (usuarios) / REST (terceros)
                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              MONOLITO MODULAR (NestJS)                       ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   Auth   ‚îÇ  ‚îÇ   IoT    ‚îÇ  ‚îÇ Booking  ‚îÇ  ‚îÇ  Smart   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Module  ‚îÇ  ‚îÇ   Core   ‚îÇ  ‚îÇ  Module  ‚îÇ  ‚îÇ  Access  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ      Shared Infrastructure Layer                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (Database, Cache, gRPC Client, Monitoring)          ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚îÇ gRPC (Bidireccional, Port 50051)
                       ‚îÇ - Monolito env√≠a comandos
                       ‚îÇ - Devices notifica eventos
                       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              DEVICES SERVICE (Golang)                        ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ         WRITE SIDE (Commands - Alta Frecuencia)     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  TCP ‚îÄ‚îê                                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  WS  ‚îÄ‚î§‚Üí Event Queue (Redis) ‚Üí Batch Processor     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  gRPC‚îÄ‚îò                                             ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                          ‚Üì                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ         READ SIDE (Queries - Optimizado)            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Redis Cache ‚Üí TimescaleDB (raw + aggregates)      ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
                          ‚îÇ TCP/WebSocket (Ports 5000-5010)
                          ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              DISPOSITIVOS IoT                                ‚îÇ
‚îÇ  (Controladores, Sensores, Actuadores, Cerraduras)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Flujo de datos explicado**:

1. **Usuario ‚Üí Monolito**: GraphQL para usuarios finales, REST para terceros
2. **Monolito ‚Üí Devices**: gRPC para enviar comandos y recibir notificaciones
3. **Dispositivos ‚Üí Devices**: TCP/WebSocket para enviar eventos
4. **Devices ‚Üí Monolito**: gRPC stream para notificar cambios importantes

### 2.2 Servicio Monolito Modular

**Arquitectura Base**: Hexagonal (Puertos y Adaptadores)

**Principios**:
- **Dominio en el centro**: L√≥gica de negocio pura, sin dependencias externas
- **Puertos (interfaces)**: Definen contratos que el dominio necesita
- **Adaptadores**: Implementaciones concretas de los puertos (DB, HTTP, gRPC)

**Estructura por capas**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     HTTP/GraphQL Adapters           ‚îÇ ‚Üê Entrada (controllers, resolvers)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Application Layer                ‚îÇ ‚Üê Casos de uso (use-cases)
‚îÇ  (Orquesta el dominio)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Domain Layer                     ‚îÇ ‚Üê L√≥gica de negocio pura
‚îÇ  (Entities, Value Objects, Services) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Infrastructure Adapters          ‚îÇ ‚Üê Salida (DB, cache, gRPC)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Caracter√≠sticas clave**:
- **Separaci√≥n clara**: Cada m√≥dulo es independiente
- **Bajo acoplamiento**: M√≥dulos se comunican via eventos o interfaces
- **Alta cohesi√≥n**: Todo lo relacionado a una funcionalidad est√° junto
- **Testeable**: Domain layer sin dependencias, f√°cil de testear

**Patrones de Dise√±o aplicados**:
- **Repository Pattern**: Abstracci√≥n de persistencia
- **Use Case Pattern**: Un caso de uso = una acci√≥n del usuario
- **Adapter Pattern**: Adapta interfaces externas al dominio
- **Singleton Pattern**: Para servicios compartidos (ej: Redis client)

**Ventaja de m√≥dulos**:
```typescript
// Cada m√≥dulo es independiente
modules/
  ‚îú‚îÄ‚îÄ auth/        ‚Üê Puede extraerse a microservicio
  ‚îú‚îÄ‚îÄ iot-core/    ‚Üê Puede extraerse a microservicio
  ‚îî‚îÄ‚îÄ booking/     ‚Üê Puede extraerse a microservicio
```

### 2.3 Servicio de Devices/Connection

**Arquitectura Base**: CQRS (Command Query Responsibility Segregation)

**Por qu√© CQRS aqu√≠**:
- Write side: Recibe 100+ eventos/seg, necesita ser ultra r√°pido
- Read side: Consultas complejas con agregaciones, necesita estar optimizado
- Son dos problemas completamente diferentes ‚Üí dos soluciones optimizadas

**Write Side (Commands)**:
```
Caracter√≠sticas:
- Prioridad: Latencia (< 10ms)
- Operaci√≥n: INSERT simple
- Volumen: Alto (100+ ops/seg)
- Estrategia: Queue + batch processing
```

**Read Side (Queries)**:
```
Caracter√≠sticas:
- Prioridad: Resultado completo y correcto
- Operaci√≥n: SELECT con agregaciones
- Volumen: Medio (usuarios consultando)
- Estrategia: Cache agresivo + continuous aggregates
```

**Patrones de Dise√±o aplicados**:
- **Command Pattern**: Cada comando es un objeto
- **Abstract Factory**: Crea handlers seg√∫n protocolo (TCP, WebSocket, Modbus)
- **Observer Pattern**: Notifica cambios a subscribers (monolito)
- **Circuit Breaker**: Protege contra fallos en cascada

**Ventaja de Go para este servicio**:
```go
// Goroutines permiten manejar miles de conexiones f√°cilmente
for {
    conn, _ := listener.Accept()
    go handleConnection(conn) // Nueva goroutine por conexi√≥n
}
```

### 2.4 Comunicaci√≥n entre Servicios

**Canal Principal**: gRPC bidireccional

**Flujos de comunicaci√≥n**:

1. **Monolito ‚Üí Devices (Comandos)**:
```
Usuario clickea "Encender luz"
‚Üí Monolito: CreateDeviceCommand use-case
‚Üí gRPC call: SendDeviceCommand()
‚Üí Devices: Encola comando y lo env√≠a al dispositivo
‚Üí Response: Command acknowledged
```

2. **Devices ‚Üí Monolito (Notificaciones)**:
```
Dispositivo reporta "Alarma activada"
‚Üí Devices: Procesa evento
‚Üí gRPC stream: NotifyDeviceEvent()
‚Üí Monolito: Recibe notificaci√≥n en tiempo real
‚Üí Acci√≥n: Env√≠a notificaci√≥n push a usuarios
```

3. **Monolito ‚Üí Devices (Queries)**:
```
Usuario abre dashboard de consumo
‚Üí Monolito: GetDeviceAnalytics use-case
‚Üí gRPC call: GetDeviceEvents()
‚Üí Devices: Query a cache/TimescaleDB
‚Üí Response: Datos agregados
```

**Por qu√© NO usamos message queue (RabbitMQ, Kafka)**:
- Para comunicaci√≥n request-response, gRPC es m√°s simple
- Message queue a√±adir√≠a complejidad innecesaria
- gRPC ya tiene retry, timeout, load balancing built-in
- Consideraremos message queue en el futuro si necesitamos:
  - Event sourcing completo
  - M√∫ltiples consumers del mismo evento
  - Replay de eventos hist√≥ricos

---

## 3. Stack Tecnol√≥gico

### 3.1 Monolito Modular (NestJS)

| Categor√≠a | Tecnolog√≠a | Versi√≥n | Justificaci√≥n |
|-----------|-----------|---------|---------------|
| **Framework** | NestJS | ^10.0.0 | Framework maduro con arquitectura modular, DI nativo, excelente TypeScript support |
| **Lenguaje** | TypeScript | ^5.0.0 | Type safety, mejor DX, refactoring seguro |
| **Servidor HTTP** | Fastify | ^4.0.0 | 2x m√°s r√°pido que Express, mejor para high throughput |
| **Base de Datos** | PostgreSQL | 15+ | ACID, mature, excelente para datos relacionales |
| **ORM** | Prisma | ^5.0.0 | Type-safe queries, migraciones autom√°ticas, excelente DX |
| **Cach√©** | Redis | 7.0+ | In-memory store, soporte para pub/sub, persistencia opcional |
| **Autenticaci√≥n** | JWT + Passport | - | Est√°ndar industry, stateless, f√°cil de escalar |
| **Hashing** | Argon2 | latest | M√°s seguro que bcrypt, resistant a GPU attacks |
| **API** | Apollo Server | ^4.0.0 | GraphQL server maduro, buena integraci√≥n con NestJS |
| **Comunicaci√≥n** | @grpc/grpc-js | ^1.9.0 | Cliente gRPC oficial para Node.js |
| **Seguridad** | Helmet + Throttler | latest | Headers de seguridad + rate limiting |
| **Validaci√≥n** | class-validator | ^0.14.0 | Validaci√≥n declarativa con decorators |
| **Monitoreo** | New Relic | ^11.0.0 | APM completo, traces distribuidos, dashboards |
| **Testing** | Jest | ^29.0.0 | Testing framework est√°ndar para Node.js |

**Por qu√© NestJS y no Express puro**:
- Arquitectura modular built-in
- Dependency injection nativo
- Soporte first-class para TypeScript
- Decorators para reducir boilerplate
- Ecosystem maduro (GraphQL, gRPC, WebSockets)

**Por qu√© Fastify y no Express**:
- Performance: ~2x m√°s requests/segundo
- Schema-based validation built-in
- Async/await nativo
- Plugin architecture superior

**Por qu√© Prisma y no TypeORM**:
- Type safety superior (genera tipos desde schema)
- Migraciones m√°s confiables
- Query performance mejor
- DX (Developer Experience) excelente

### 3.2 Devices Service (Golang)

| Categor√≠a | Tecnolog√≠a | Versi√≥n | Justificaci√≥n |
|-----------|-----------|---------|---------------|
| **Lenguaje** | Go | 1.21+ | Concurrencia nativa, performance, simplicidad |
| **Base de Datos** | PostgreSQL + TimescaleDB | 15+ / 2.13+ | Series temporales optimizadas, compatible con PG |
| **ORM** | GORM | v1.25.0+ | ORM maduro para Go, migrations, associations |
| **Framework Web** | Echo | v4.11.0+ | Ligero, r√°pido, middleware robusto |
| **Redis Client** | go-redis | v9.0.0+ | Cliente Redis oficial para Go |
| **gRPC** | google.golang.org/grpc | v1.58.0+ | Implementaci√≥n oficial de gRPC |
| **Protocol Buffers** | protobuf | v1.31.0+ | Serializaci√≥n eficiente |
| **Circuit Breaker** | sony/gobreaker | v2.0.0+ | Patr√≥n circuit breaker battle-tested |
| **Logging** | zerolog | v1.31.0+ | Structured logging, zero allocation |
| **Validaci√≥n** | validator | v10.15.0+ | Struct validation con tags |
| **Testing** | testify | v1.8.0+ | Assertions y mocks |
| **Monitoreo** | go-agent (New Relic) | v3.25.0+ | APM para Go (opcional) |

**Por qu√© Echo y no Gin**:
- Middleware m√°s flexible
- Mejor manejo de errors
- WebSocket support built-in
- Menos opinionated, m√°s control

**Por qu√© GORM y no sqlx**:
- Migrations autom√°ticas
- Associations (foreign keys) built-in
- Hooks para lifecycle events
- Balance entre abstracci√≥n y control

**Por qu√© zerolog**:
- Zero allocation logging (importante para performance)
- Structured logging (JSON output)
- Sampling support
- Context-aware logging

### 3.3 Base de Datos - TimescaleDB

**Decisi√≥n cr√≠tica**: PostgreSQL + extensi√≥n TimescaleDB

**Por qu√© TimescaleDB y NO PostgreSQL plain**:

1. **Particionamiento autom√°tico por tiempo**:
```sql
-- TimescaleDB hace esto autom√°ticamente
SELECT create_hypertable('device_events', 'time');
-- Crea particiones (chunks) autom√°ticas cada 7 d√≠as
```

2. **Compresi√≥n de datos antiguos**:
```sql
-- Comprime datos > 7 d√≠as (ahorra 90% de espacio)
ALTER TABLE device_events SET (timescaledb.compress);
-- Queries siguen funcionando, transparente para la app
```

3. **Continuous Aggregates** (pre-c√°lculos autom√°ticos):
```sql
-- Vista materializada que se actualiza sola
CREATE MATERIALIZED VIEW device_events_hourly
WITH (timescaledb.continuous) AS
SELECT 
  time_bucket('1 hour', time) as hour,
  device_id,
  AVG(value) as avg_value
FROM device_events
GROUP BY hour, device_id;

-- Query instant√°neo vs minutos en PostgreSQL plain
SELECT * FROM device_events_hourly WHERE hour >= NOW() - INTERVAL '7 days';
```

4. **Retenci√≥n autom√°tica**:
```sql
-- Elimina datos > 2 a√±os autom√°ticamente
SELECT add_retention_policy('device_events', INTERVAL '2 years');
```

**Comparaci√≥n de performance**:
```
Query: Promedio de consumo por hora, √∫ltimos 30 d√≠as

PostgreSQL plain:
- Scan: 260M rows
- Tiempo: ~45 segundos
- Uso de RAM: 4GB

TimescaleDB (con continuous aggregate):
- Scan: 720 rows (30 d√≠as √ó 24 horas)
- Tiempo: ~50ms
- Uso de RAM: 10MB

Mejora: 900x m√°s r√°pido
```

**Instalaci√≥n en RDS**:
```sql
-- TimescaleDB es una extensi√≥n de PostgreSQL
CREATE EXTENSION IF NOT EXISTS timescaledb;

-- Verificar
SELECT extname, extversion FROM pg_extension WHERE extname = 'timescaledb';
```

### 3.4 Infraestructura Base

| Servicio | Tecnolog√≠a | Justificaci√≥n |
|----------|-----------|---------------|
| **Contenedores** | Docker | Est√°ndar industry, portabilidad |
| **Orquestaci√≥n** | AWS ECS Fargate | Serverless containers, sin gesti√≥n de servers |
| **Base de Datos Managed** | AWS RDS PostgreSQL | Backups autom√°ticos, Multi-AZ, patches autom√°ticos |
| **Cach√© Managed** | AWS ElastiCache Redis | Redis gestionado, cluster mode, auto-failover |
| **Load Balancer** | AWS ALB | Layer 7 LB, SSL termination, health checks |
| **CDN** | CloudFront | Baja latencia global, DDoS protection |
| **Secrets** | AWS Secrets Manager | Rotaci√≥n autom√°tica, encriptado, audit logs |
| **Logs** | CloudWatch + New Relic | Logs centralizados, b√∫squeda, alertas |
| **CI/CD** | GitHub Actions | Integraci√≥n con GitHub, free para repos p√∫blicos |

**Por qu√© ECS Fargate y no Kubernetes**:
- Menos complejidad operacional
- Sin gesti√≥n de nodes
- Auto-scaling m√°s simple
- Suficiente para nuestro caso de uso
- Consideraremos K8s cuando tengamos 20+ microservicios

**Por qu√© RDS y no PostgreSQL en EC2**:
- Backups autom√°ticos
- Multi-AZ con failover autom√°tico (< 60 segundos)
- Patches de seguridad autom√°ticos
- Read replicas f√°ciles
- Point-in-time recovery
- Monitoring built-in

**Por qu√© CloudWatch + New Relic y no solo CloudWatch**:
- CloudWatch: Logs e infraestructura (incluido con AWS)
- New Relic: APM, distributed tracing, mejor UX
- Combinaci√≥n da visibilidad completa

---

**üìÑ Contin√∫a en**: `04-06_Modulos_Datos_Reglas.md`